{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMY-EUX1ermx"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import tensorflow as tf\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "\n",
        "def normalized_adj(adj):\n",
        "    # Convert the adjacency matrix to a TensorFlow tensor\n",
        "    adj = tf.convert_to_tensor(adj, dtype=tf.float32)\n",
        "\n",
        "    # Calculate the row sum\n",
        "    rowsum = tf.reduce_sum(adj, axis=1)\n",
        "\n",
        "    # Calculate the inverse square root of the row sums\n",
        "    d_inv_sqrt = 1.0 / tf.sqrt(tf.maximum(rowsum, 1e-12))  # Add a small constant to avoid division by zero\n",
        "\n",
        "    # Create a diagonal matrix of the inverse square root\n",
        "    d_mat_inv_sqrt = tf.linalg.diag(d_inv_sqrt)\n",
        "\n",
        "    # Calculate the normalized adjacency matrix\n",
        "    normalized_adj = tf.matmul(tf.matmul(d_mat_inv_sqrt,adj), d_mat_inv_sqrt)\n",
        "\n",
        "    return normalized_adj\n",
        "\n",
        "def convert_sparse_matrix_to_sparse_tensor(X):\n",
        "    coo = X.tocoo()\n",
        "    indices = np.mat([coo.row, coo.col]).transpose()\n",
        "    return tf.SparseTensor(indices, coo.data, coo.shape)\n",
        "def sparse_to_tuple(mx):\n",
        "    mx = mx.tocoo()\n",
        "    coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "    L = tf.SparseTensor(coords, mx.data, mx.shape)\n",
        "    return tf.sparse.reorder(L)\n",
        "\n",
        "def calculate_laplacian(adj, lambda_max=1):\n",
        "    adj = normalized_adj(adj + tf.eye(adj.shape[0], dtype=tf.float32))\n",
        "    return adj\n",
        "\n",
        "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = tf.compat.v1.random_uniform([input_dim, output_dim], minval=-init_range,\n",
        "                            maxval=init_range, dtype=tf.float32)\n",
        "\n",
        "    return tf.Variable(initial,name=name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXl2fJk_exFp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "\n",
        "def preprocess_data(data, time_len, rate, seq_len, pre_len):\n",
        "    train_size = int(time_len * rate)\n",
        "    train_data = data[0:train_size]\n",
        "    test_data = data[train_size:time_len]\n",
        "\n",
        "    trainX, trainY, testX, testY = [], [], [], []\n",
        "    for i in range(len(train_data) - seq_len - pre_len):\n",
        "        a = train_data[i: i + seq_len + pre_len]\n",
        "        trainX.append(a[0 : seq_len])\n",
        "        trainY.append(a[seq_len : seq_len + pre_len])\n",
        "    for i in range(len(test_data) - seq_len -pre_len):\n",
        "        b = test_data[i: i + seq_len + pre_len]\n",
        "        testX.append(b[0 : seq_len])\n",
        "        testY.append(b[seq_len : seq_len + pre_len])\n",
        "\n",
        "    trainX1 = np.array(trainX)\n",
        "    trainY1 = np.array(trainY)\n",
        "    testX1 = np.array(testX)\n",
        "    testY1 = np.array(testY)\n",
        "    return trainX1, trainY1, testX1, testY1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pvWIvqjjMvh"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python2\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Apr 27 09:39:22 2018\n",
        "\n",
        "@author: lhfcitylab\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_result(test_result,test_label1):\n",
        "    ##all test result visualization\n",
        "    fig1 = plt.figure(figsize=(11,2))\n",
        "    a_pred = test_result\n",
        "    print(a_pred.shape)\n",
        "    a_true = test_label1\n",
        "    print(a_pred.shape)\n",
        "    plt.plot(a_pred,'r-',label='Our Model')\n",
        "    plt.plot(a_true,'b-',label='Ground Truth')\n",
        "    plt.legend(loc='best',fontsize=10)\n",
        "    labels = [\"06/19/2012\",\"06/20/2012\",\"06/21/2012\",\"06/24/2012\",\"06/25/2012\",\"06/26/2012\",\"06/27/2012\",\"06/28/2012\"]\n",
        "    plt.xlabel(\"Time(MM/DD/YYYY)\")\n",
        "    plt.ylabel(\"Traffic speed\")\n",
        "    plt.xticks([0,320,640,960,1280,1600,1920,2240],labels)\n",
        "    fig1 = plt.figure(figsize=(7,1.5))\n",
        "    a_pred = test_result[0:96]\n",
        "    a_true = test_label1[0:96]\n",
        "    labels  = [\"00.00\", \"03.00\", \"06.00\", \"09.00\", \"12.00\",\"15.00\",\"18.00\",\"21.00\",\"24.00\"]\n",
        "    plt.xticks([0,12,24,36,48,60,72,84,96],labels)\n",
        "    plt.ylabel(\"Traffic speed\")\n",
        "    plt.xlabel(\"Time(hour)\")\n",
        "    plt.plot(a_pred,'r-',label=\"Our Model\")\n",
        "    plt.plot(a_true,'b-',label=\"Ground Truth\")\n",
        "    plt.legend(loc='best',fontsize=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNJs5xMHe4od"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.compat.v1.nn.rnn_cell import RNNCell\n",
        "\n",
        "class pstgCell(RNNCell):\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def __init__(self, num_units,adj, node_embed,num_nodes, input_size=None,\n",
        "                 act=tf.nn.tanh, reuse=None):\n",
        "\n",
        "        super(tgcnCell, self).__init__(_reuse=reuse)\n",
        "        self._act = act\n",
        "        self._nodes = num_nodes\n",
        "        self._units = num_units\n",
        "        self._adj = (calculate_laplacian(adj))\n",
        "        self.node_embed = node_embed\n",
        "\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return self._nodes * self._units\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return self._units\n",
        "\n",
        "    def __call__(self, inputs,state, scope=None):\n",
        "\n",
        "\n",
        "        with tf.compat.v1.variable_scope(scope or \"tgcn\"):\n",
        "            with tf.compat.v1.variable_scope(\"gates\"):\n",
        "                value = tf.nn.sigmoid(self._gc(inputs, state, 2 * self._units, bias=1.0, scope=scope))\n",
        "                r, u = tf.split(value=value, num_or_size_splits=2, axis=1)\n",
        "\n",
        "            with tf.compat.v1.variable_scope(\"candidate\"):\n",
        "                r_state = r * state\n",
        "                c = self._act(self._gc(inputs, r_state, self._units, scope=scope))\n",
        "            new_h = u * state + (1 - u) * c\n",
        "        return new_h, new_h\n",
        "\n",
        "\n",
        "    def _gc(self, inputs, state, output_size, bias=0.0, scope=None):\n",
        "        emd_dim = 10\n",
        "        inputs = tf.expand_dims(inputs, 2)\n",
        "        state = tf.reshape(state, (-1, self._nodes, self._units))\n",
        "        x_s = tf.concat([inputs, state], axis=2)\n",
        "        input_size = x_s.get_shape()[2]\n",
        "        weights_pool = tf.Variable(tf.random.normal([ emd_dim , input_size, output_size]), trainable=True)\n",
        "        bias_pool = tf.Variable(tf.random.normal([ emd_dim , output_size]), trainable=True)\n",
        "        weight = tf.einsum('nd,dio->nio',  self.node_embed, weights_pool)\n",
        "        bias = tf.matmul( self.node_embed, bias_pool)\n",
        "        x0 = tf.transpose(x_s, perm=[1, 2, 0])\n",
        "        x0 = tf.reshape(x0, shape=[self._nodes, -1])\n",
        "\n",
        "        scope = tf.compat.v1.get_variable_scope()\n",
        "        with tf.compat.v1.variable_scope(scope):\n",
        "            # for m in self._adj:\n",
        "            x1 = tf.matmul(self._adj, x0)\n",
        "            # print(\"xx1\",x1.shape)\n",
        "            x = tf.reshape(x1, shape=[self._nodes, input_size,-1])\n",
        "            x = tf.transpose(x,perm=[2,0,1])\n",
        "            x = tf.einsum('bni,nio->bno', x, weight)+bias\n",
        "            x = tf.reshape(x, shape=[-1, self._nodes, output_size])\n",
        "            x = tf.reshape(x, shape=[-1, self._nodes * output_size])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_adjacency(X,adj,node):\n",
        "  input_len = X.shape[1]\n",
        "  X = tf.matmul(X,adj)\n",
        "  x1 = X[:,0,:]\n",
        "  x1 = tf.reshape(x1, shape=[-1,node])\n",
        "  x1 = tf.transpose(x1, perm=[1,0])\n",
        "  x2 = X[:,1,:]\n",
        "  x2 = tf.reshape(x2, shape=[-1,node])\n",
        "  trans_prob = tf.matmul(x1,x2)\n",
        "  sum_prob = tf.reduce_sum(trans_prob, axis=1)\n",
        "  tiled_sum_prob = tf.tile(tf.expand_dims(sum_prob, axis=1), [1, node])\n",
        "  transitional_probability = tf.math.divide_no_nan(trans_prob, tiled_sum_prob)\n",
        "  remain_input_len = input_len-2\n",
        "  if input_len>2:\n",
        "      for i in range(remain_input_len):\n",
        "         x3 = X[:,i+2,:]\n",
        "         x3 = tf.reshape(x3, shape=[-1,node])\n",
        "         x3 = tf.transpose(x3, perm=[1,0])\n",
        "         trans_prob = tf.matmul(x3,x2)\n",
        "         trans_prob = tf.matmul(transitional_probability,trans_prob)\n",
        "         sum_prob = tf.reduce_sum(trans_prob, axis=1)\n",
        "         tiled_sum_prob = tf.tile(tf.expand_dims(sum_prob, axis=1), [1, node])\n",
        "         transitional_probability = tf.math.divide_no_nan(trans_prob, tiled_sum_prob)\n",
        "         x3 = tf.transpose(x3, perm=[1,0])\n",
        "         x2 = x3\n",
        "  return transitional_probability"
      ],
      "metadata": {
        "id": "2P0WBbk9uxVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import numpy.linalg as la\n",
        "tf.compat.v1.reset_default_graph()\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import time\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "train_rate =  0.8\n",
        "seq_len = 12\n",
        "output_dim = pre_len =12\n",
        "lr = 0.001\n",
        "training_epoch =500\n",
        "batch_size = 32\n",
        "gru_units = 64\n",
        "\n",
        "los_adj = pd.read_csv('https://raw.githubusercontent.com/AmitRoy7781/USTGCN/master/PeMSD7/PeMSD7_adj.csv',header=None)\n",
        "adj = np.mat(los_adj)\n",
        "adj = adj[:200,:200]\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/AmitRoy7781/USTGCN/master/PeMSD7/PeMSD7_speed.csv')\n",
        "data1 =np.mat(data,dtype=np.float32)\n",
        "data1 = data1[:,:200]\n",
        "print(\"data\",data1.shape)\n",
        "time_len = data1.shape[0]\n",
        "num_nodes = data1.shape[1]\n",
        "emd_dim = 10\n",
        "#### normalization\n",
        "max_value = np.max(data1)\n",
        "data1  = data1/max_value\n",
        "trainX, trainY, testX, testY = preprocess_data(data1, time_len, train_rate, seq_len, pre_len)\n",
        "print(testY.shape)\n",
        "totalbatch = int(trainX.shape[0]/batch_size)\n",
        "training_data_count = len(trainX)\n",
        "\n",
        "def PSTG(_X, weights, biases):\n",
        "    node_embeddings = tf.Variable(tf.random.normal((num_nodes, emd_dim)), trainable=True)\n",
        "    weight = tf.einsum('nd,dio->nio',  node_embeddings, weights)\n",
        "    bias = tf.matmul(node_embeddings, biases)\n",
        "    Ad_adjt = dynamic_adjacency(_X,adj,num_nodes)\n",
        "    cell_1 = pstgCell(gru_units,Ad_adjt, node_embeddings,num_nodes=num_nodes)\n",
        "    cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True)\n",
        "    _X = tf.unstack(_X, axis=1)\n",
        "    outputs, states = tf.compat.v1.nn.static_rnn(cell, _X, dtype=tf.float32)\n",
        "    out = tf.concat(outputs, axis=0)\n",
        "    out = tf.reshape(out, shape=[seq_len,-1,num_nodes,gru_units])\n",
        "    out = tf.transpose(out, perm=[1,0,2,3])\n",
        "    last_output,alpha = self_attention1(out, weight_att, bias_att)\n",
        "    output = tf.einsum('bni,nio->bno', last_output, weight)+bias\n",
        "    output = tf.reshape(output,shape=[-1,num_nodes,pre_len])\n",
        "    output = tf.transpose(output, perm=[0,2,1])\n",
        "    output = tf.reshape(output, shape=[-1,num_nodes])\n",
        "\n",
        "    return output, outputs, states, alpha\n",
        "\n",
        "def self_attention1(x, weight_att,bias_att):\n",
        "    x = tf.matmul(tf.reshape(x,[-1,gru_units]),weight_att['w1']) + bias_att['b1']\n",
        "    f = tf.matmul(tf.reshape(x, [-1, num_nodes]), weight_att['w2']) + bias_att['b2']\n",
        "    g = tf.matmul(tf.reshape(x, [-1, num_nodes]), weight_att['w2']) + bias_att['b2']\n",
        "    h = tf.matmul(tf.reshape(x, [-1, num_nodes]), weight_att['w2']) + bias_att['b2']\n",
        "\n",
        "    f1 = tf.reshape(f, [-1,seq_len])\n",
        "    g1 = tf.reshape(g, [-1,seq_len])\n",
        "    h1 = tf.reshape(h, [-1,seq_len])\n",
        "    s = g1 * f1\n",
        "\n",
        "    beta = tf.nn.softmax(s, axis=-1)  # attention map\n",
        "    context = tf.expand_dims(beta,2) * tf.reshape(x,[-1,seq_len,num_nodes])\n",
        "\n",
        "    context = tf.transpose(context,perm=[0,2,1])\n",
        "    return context, beta\n",
        "###### placeholders ######\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "inputs = tf.compat.v1.placeholder(tf.float32, shape=[None, seq_len, num_nodes])\n",
        "labels = tf.compat.v1.placeholder(tf.float32, shape=[None, pre_len, num_nodes])\n",
        "\n",
        "# weights\n",
        "weights_pool = tf.Variable(tf.random.normal([ emd_dim , seq_len, pre_len]), trainable=True)\n",
        "bias_pool = tf.Variable(tf.random.normal([ emd_dim , pre_len]), trainable=True)\n",
        "weight_att={\n",
        "    'w1':tf.Variable(tf.compat.v1.random_normal([gru_units,1], stddev=0.1),name='att_w1'),\n",
        "    'w2':tf.Variable(tf.compat.v1.random_normal([num_nodes,1], stddev=0.1),name='att_w2')}\n",
        "bias_att = {\n",
        "    'b1': tf.Variable(tf.compat.v1.random_normal([1]),name='att_b1'),\n",
        "    'b2': tf.Variable(tf.compat.v1.random_normal([1]),name='att_b2')}\n",
        "\n",
        "pred,ttto,ttts,alpha = PSTG(inputs, weights_pool, bias_pool)\n",
        "\n",
        "y_pred = pred\n",
        "\n",
        "\n",
        "\n",
        "###### optimizer ######\n",
        "lambda_loss = 0.0015\n",
        "Lreg = lambda_loss * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.compat.v1.trainable_variables())\n",
        "label = tf.reshape(labels, [-1,num_nodes])\n",
        "##loss\n",
        "loss = tf.reduce_mean(tf.nn.l2_loss(y_pred-label) + Lreg)\n",
        "##rmse\n",
        "error = tf.sqrt(tf.reduce_mean(tf.square(y_pred-label)))\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(lr).minimize(loss)\n",
        "\n",
        "###### Initialize session ######\n",
        "variables = tf.compat.v1.global_variables()\n",
        "saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n",
        "#sess = tf.Session()\n",
        "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\n",
        "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
        "sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "\n",
        "###### evaluation ######\n",
        "def evaluation(a,b):\n",
        "    rmse = math.sqrt(mean_squared_error(a,b))\n",
        "    mae = mean_absolute_error(a, b)\n",
        "    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n",
        "    r2 = 1-((a-b)**2).sum()/((a-a.mean())**2).sum()\n",
        "    var = 1-(np.var(a-b))/np.var(a)\n",
        "    return rmse, mae, 1-F_norm, r2, var\n",
        "\n",
        "def extract_batch_size(_train, step, batch_size):\n",
        "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data.\n",
        "    shape = list(_train.shape)\n",
        "    shape[0] = batch_size\n",
        "    batch_s = np.empty(shape)\n",
        "    for i in range(batch_size):\n",
        "        # Loop index\n",
        "        index = ((step-1)*batch_size + i) % len(_train)\n",
        "        batch_s[i] = _train[index]\n",
        "    return batch_s\n",
        "\n",
        "\n",
        "x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], []\n",
        "test_loss,test_rmse,test_mae,test_acc,test_r2,test_var,test_pred = [],[],[],[],[],[],[]\n",
        "\n",
        "for epoch in range(training_epoch):\n",
        "    for m in range(totalbatch):\n",
        "        mini_batch = trainX[m * batch_size : (m+1) * batch_size]\n",
        "        mini_label = trainY[m * batch_size : (m+1) * batch_size]\n",
        "        _, loss1, rmse1, train_output, alpha1 = sess.run([optimizer, loss, error, y_pred, alpha],\n",
        "                                                 feed_dict = {inputs:mini_batch, labels:mini_label})\n",
        "        batch_loss.append(loss1)\n",
        "        batch_rmse.append(rmse1 * max_value)\n",
        "\n",
        "     # Test completely at every epoch\n",
        "    loss2, rmse2, test_output = sess.run([loss, error, y_pred],\n",
        "                                         feed_dict = {inputs:testX, labels:testY})\n",
        "    test_label = np.reshape(testY,[-1,num_nodes])\n",
        "    rmse, mae, acc, r2_score, var_score = evaluation(test_label, test_output)\n",
        "    test_label1 = test_label * max_value\n",
        "    test_output1 = test_output * max_value\n",
        "    test_loss.append(loss2)\n",
        "    test_rmse.append(rmse * max_value)\n",
        "    test_mae.append(mae * max_value)\n",
        "\n",
        "    test_acc.append(acc)\n",
        "    test_r2.append(r2_score)\n",
        "    test_var.append(var_score)\n",
        "    test_pred.append(test_output1)\n",
        "\n",
        "    print('Iter:{}'.format(epoch),\n",
        "          'train_rmse:{:.4}'.format(batch_rmse[-1]),\n",
        "          'test_loss:{:.4}'.format(loss2),\n",
        "          'test_rmse:{:.4}'.format(rmse),\n",
        "          'test_acc:{:.4}'.format(acc))\n",
        "\n",
        "time_end = time.time()\n",
        "print(time_end-time_start,'s')\n",
        "\n",
        "\n",
        "index = test_rmse.index(np.min(test_rmse))\n",
        "test_result = test_pred[index]\n",
        "var = pd.DataFrame(test_result)\n",
        "test_result = np.reshape(test_result,[-1,pre_len,num_nodes])\n",
        "test_label1 = np.reshape(test_label1,[-1,pre_len,num_nodes])\n",
        "plot_result(test_result[:,0,0],test_label1[:,0,0])\n",
        "\n",
        "print('min_rmse:%r'%(np.min(test_rmse)),\n",
        "      'min_mae:%r'%(test_mae[index]),\n",
        "      'max_acc:%r'%(test_acc[index]),\n",
        "      'r2:%r'%(test_r2[index]),\n",
        "      'var:%r'%test_var[index])"
      ],
      "metadata": {
        "id": "VD00zo2NnfBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}